#!/usr/bin/env python
# coding: utf-8

# In[2]:


pip install keras


# In[5]:


pip install tensorflow


# In[27]:


#Data Handling and Manipulation
import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)  

#Statistical Analysis
from scipy import stats

#Plotting and Visualization
import matplotlib.pyplot as plt
import matplotlib.pylab as plt
plt.rcParams['figure.dpi'] = 120



#Machine Learning
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier

import tensorflow as tf
from tensorflow import keras


# In[7]:


#Load customer loans dataset
data_loans = pd.read_csv('CustomerLoans.csv',sep=',')
data_loans.head()


# In[8]:


#Load demographics dataset
data_demographics = pd.read_csv('Demographics.csv',sep=',')
data_demographics.head()


# In[9]:


#Examine Missing Data
data_loans.isna().sum()
data_demographics.isna().sum()


# In[10]:


#Examin outliers in customer loans dataset 
num_cols_loans = ['Income','CreditScore','Debt','LoanTerm','InterestRate','CreditIncidents','HomeValue','LoanAmount']
data_loans[(np.abs(stats.zscore(data_loans[num_cols_loans])) > 3).all(axis=1)]


# In[11]:


#Examin outliers in demographics dataset 
num_cols_customer = ['Age','Income','CreditScore','HouseholdSize','MedianHomeValue','Debt']
data_demographics[(np.abs(stats.zscore(data_demographics[num_cols_customer])) > 3).all(axis=1)]


# In[12]:


#Providing descriptive statistics 
data_loans[num_cols_loans].describe()
data_demographics[num_cols_customer].describe()


# In[13]:


#Data histograms for customer loans 
data_loans[num_cols_loans].hist()
plt.tight_layout()


# In[14]:


#Data histograms for demographics
data_demographics[num_cols_customer].hist()
plt.tight_layout()


# In[16]:


#Joining datasets together
cols_to_use =data_demographics.columns.difference(data_loans.columns).tolist()
cols_to_use.append('custid')
df = data_loans.merge(data_demographics[cols_to_use], on = 'custid')
df.head()


# In[17]:


#Create features 
num_features = ['Income','CreditScore','Debt','LoanTerm','InterestRate','CreditIncidents','HomeValue','LoanAmount',
               'Lat','Long','MedianHomeValue','MedianHouseholdIncome']
df_features = df[num_features]
# df_product_type = pd.get_dummies(df.ProductType,prefix='ProductType')
# df_features = pd.concat([df_features,df_product_type],axis=1)
features = df_features.values
targets = np.argmax(pd.get_dummies(df.LoanStatus).values,axis=1)
# print(features.shape)


# In[18]:


#Scaling data 
scaler=MinMaxScaler()
X = scaler.fit_transform(features)


# In[19]:


#Neural Network Model Creation

def create_model(input_shape):
    model = Sequential()
    model.add(Dense(128, input_dim=input_shape, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    return model


# In[20]:


#Training and evaluate Model 

def train_and_evaluate__model(model, data_train, labels_train, data_test, labels_test):
    history = model.fit(data_train,labels_train,validation_data=(data_test,labels_test),epochs=30,batch_size=128)
    val_acc = history.history['val_accuracy'][-1]  # change this line if running on windows to: val_acc = history.history['val_accuracy'][-1] 
    return val_acc, history


# In[21]:


#K-Fold cross validation 

scores = []
models = []
historys = []
num_splits = 3
kf = KFold(n_splits=num_splits)
kf.get_n_splits(X)
input_shape = X.shape[1]

fold = 0
for train_index, test_index in kf.split(X):
    print("Running fold {}".format(fold))
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = targets[train_index], targets[test_index]
    model = create_model(input_shape)
    score, history = train_and_evaluate__model(model,X_train,y_train,X_test,y_test)
    scores.append(score)
    models.append(model)
    historys.append(history)
    fold += 1
    
print('\n\nEstimated Accuracy ' , (np.round(np.mean(scores),2)))


# In[22]:


#model creation after K-Fold cross validation 
X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.20, random_state=42)
model = create_model(input_shape)
history = model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=30,batch_size=128)


# In[23]:


#model performance (loss)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show();


# In[24]:


#model performance (accuracy)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model acc')
plt.ylabel('acc')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show();


# In[25]:


# save the model
model.save('my_model')


# In[31]:


# load the model
new_model = tf.keras.models.load_model('my_model')

# Check its architecture
new_model.summary()


# In[56]:


# data = np.array([43415, 658, 23878.25, 180, 0.036, 3, 179930, 71972, 31.84568, -102.36764, 94700, 44174])
# num_features = ['Income','CreditScore','Debt','LoanTerm','InterestRate','CreditIncidents','HomeValue','LoanAmount',
#                'Lat','Long','MedianHomeValue','MedianHouseholdIncome']
data2 = np.array([4735, 0, 32431.1, 480, 0.035, 60, 561000, 336600, 41.50343, -74.01042, 255000, 56444])
reshape_data = np.reshape(data2, (1, 12))
pred = model.predict(reshape_data)
print(pred)





